"""
Compliance guardian for zero-trust checks, consent verification and immutable audit logs.
Ensures ethical boundaries and produces compliance artifacts for SOC 2, ISO 27001.
"""

import asyncio
import hashlib
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import ipaddress

from ..config import PentestConfig, SecurityConfig


class AuditLog:
    """Immutable audit log entry."""
    
    def __init__(self, event_type: str, event_data: Dict[str, Any], user: str = "system"):
        self.timestamp = datetime.now()
        self.event_type = event_type
        self.event_data = event_data
        self.user = user
        self.event_id = self._generate_event_id()
        self.signature = self._generate_signature()
    
    def _generate_event_id(self) -> str:
        """Generate unique event ID."""
        content = f"{self.timestamp.isoformat()}{self.event_type}{self.user}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]
    
    def _generate_signature(self) -> str:
        """Generate signature for tamper detection."""
        content = json.dumps({
            "timestamp": self.timestamp.isoformat(),
            "event_type": self.event_type,
            "event_data": self.event_data,
            "user": self.user,
            "event_id": self.event_id
        }, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "timestamp": self.timestamp.isoformat(),
            "event_type": self.event_type,
            "event_data": self.event_data,
            "user": self.user,
            "event_id": self.event_id,
            "signature": self.signature
        }
    
    def verify_integrity(self) -> bool:
        """Verify the integrity of this audit log entry."""
        expected_signature = self._generate_signature()
        return self.signature == expected_signature


class ComplianceGuardian:
    """Guardian for security, compliance, and ethical testing boundaries."""
    
    def __init__(self, config: PentestConfig):
        self.config = config
        self.security_config = config.security_config
        self.logger = logging.getLogger("compliance.guardian")
        
        self.audit_logs = []
        self.audit_log_file = Path(self.security_config.audit_log_path)
        self.consent_records = {}
        self.blocked_actions = []
        
        # Compliance frameworks
        self.compliance_mappings = {
            "SOC2": {
                "CC6.1": "Logical and physical access controls",
                "CC6.2": "System access controls", 
                "CC6.3": "Network access controls",
                "CC6.7": "Data transmission controls"
            },
            "ISO27001": {
                "A.9.1": "Access control policy",
                "A.9.4": "System access management",
                "A.13.1": "Network security management",
                "A.14.1": "Security in development"
            }
        }
        
        # Initialize audit logging
        self._initialize_audit_logging()
    
    def _initialize_audit_logging(self):
        """Initialize audit logging system."""
        # Ensure audit log directory exists
        self.audit_log_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Load existing audit logs
        if self.audit_log_file.exists():
            try:
                with open(self.audit_log_file, 'r') as f:
                    for line in f:
                        if line.strip():
                            log_data = json.loads(line.strip())
                            # Verify integrity of existing logs
                            if self._verify_log_integrity(log_data):
                                self.audit_logs.append(log_data)
                            else:
                                self.logger.warning(f"Integrity check failed for log: {log_data.get('event_id')}")
            except Exception as e:
                self.logger.error(f"Failed to load audit logs: {e}")
        
        # Log guardian initialization
        self._audit_log("GUARDIAN_INITIALIZED", {
            "zero_trust_enabled": self.security_config.enforce_zero_trust,
            "consent_required": self.security_config.require_consent,
            "audit_enabled": self.security_config.audit_log_enabled,
            "compliance_frameworks": self.security_config.compliance_frameworks
        })
    
    def _verify_log_integrity(self, log_data: Dict[str, Any]) -> bool:
        """Verify the integrity of a log entry."""
        try:
            # Recreate signature
            content = json.dumps({
                "timestamp": log_data["timestamp"],
                "event_type": log_data["event_type"],
                "event_data": log_data["event_data"],
                "user": log_data["user"],
                "event_id": log_data["event_id"]
            }, sort_keys=True)
            expected_signature = hashlib.sha256(content.encode()).hexdigest()
            return log_data["signature"] == expected_signature
        except Exception:
            return False
    
    def _audit_log(self, event_type: str, event_data: Dict[str, Any], user: str = "system"):
        """Create an immutable audit log entry."""
        if not self.security_config.audit_log_enabled:
            return
        
        audit_entry = AuditLog(event_type, event_data, user)
        self.audit_logs.append(audit_entry.to_dict())
        
        # Write to file immediately for immutability
        try:
            with open(self.audit_log_file, 'a') as f:
                f.write(json.dumps(audit_entry.to_dict()) + '\n')
                f.flush()
        except Exception as e:
            self.logger.error(f"Failed to write audit log: {e}")
    
    async def verify_setup(self) -> bool:
        """Verify that the security setup meets compliance requirements."""
        self.logger.info("Verifying security compliance setup...")
        
        verification_checks = []
        
        # Check zero-trust enforcement
        if self.security_config.enforce_zero_trust:
            verification_checks.append(("zero_trust", True, "Zero-trust enforcement enabled"))
        else:
            verification_checks.append(("zero_trust", False, "Zero-trust enforcement disabled"))
        
        # Check consent requirement
        if self.security_config.require_consent:
            verification_checks.append(("consent", True, "Consent verification required"))
        else:
            verification_checks.append(("consent", False, "Consent verification disabled"))
        
        # Check audit logging
        if self.security_config.audit_log_enabled:
            verification_checks.append(("audit_logging", True, "Audit logging enabled"))
        else:
            verification_checks.append(("audit_logging", False, "Audit logging disabled"))
        
        # Check allowed targets configuration
        if self.security_config.allowed_targets:
            verification_checks.append(("target_allowlist", True, f"Allowed targets configured: {len(self.security_config.allowed_targets)}"))
        else:
            verification_checks.append(("target_allowlist", False, "No allowed targets configured"))
        
        # Log verification results
        self._audit_log("SECURITY_VERIFICATION", {
            "checks": verification_checks,
            "passed": all(check[1] for check in verification_checks)
        })
        
        # Require all checks to pass for zero-trust
        if self.security_config.enforce_zero_trust:
            passed = all(check[1] for check in verification_checks)
            if not passed:
                failed_checks = [check[0] for check in verification_checks if not check[1]]
                self.logger.error(f"Security verification failed: {failed_checks}")
                return False
        
        self.logger.info("Security verification completed")
        return True
    
    async def verify_consent(self, target: str, user: str = "operator") -> bool:
        """Verify that consent has been granted for testing the target."""
        if not self.security_config.require_consent:
            return True
        
        self.logger.info(f"Verifying consent for target: {target}")
        
        # Check if consent has been previously recorded
        if target in self.consent_records:
            consent_data = self.consent_records[target]
            if consent_data.get("valid", False):
                self._audit_log("CONSENT_VERIFIED", {
                    "target": target,
                    "user": user,
                    "consent_id": consent_data.get("consent_id"),
                    "granted_by": consent_data.get("granted_by"),
                    "granted_at": consent_data.get("granted_at")
                })
                return True
        
        # In a real implementation, this would integrate with a consent management system
        # For demo purposes, we'll simulate consent verification
        
        # Mock consent verification - in production would require actual authorization
        mock_consent = await self._mock_consent_verification(target, user)
        
        if mock_consent:
            consent_id = hashlib.sha256(f"{target}{user}{datetime.now().isoformat()}".encode()).hexdigest()[:16]
            
            self.consent_records[target] = {
                "consent_id": consent_id,
                "granted_by": user,
                "granted_at": datetime.now().isoformat(),
                "target": target,
                "valid": True
            }
            
            self._audit_log("CONSENT_GRANTED", {
                "target": target,
                "user": user,
                "consent_id": consent_id,
                "method": "mock_verification"
            })
            
            return True
        else:
            self._audit_log("CONSENT_DENIED", {
                "target": target,
                "user": user,
                "reason": "Mock verification failed"
            })
            return False
    
    async def _mock_consent_verification(self, target: str, user: str) -> bool:
        """Mock consent verification for demo purposes."""
        # For demo, grant consent for localhost and test ranges
        try:
            # Check if it's a localhost address
            if target in ["localhost", "127.0.0.1", "::1"]:
                return True
            
            # Check if it's in test ranges
            test_ranges = [
                "192.168.0.0/16",
                "10.0.0.0/8", 
                "172.16.0.0/12"
            ]
            
            ip = ipaddress.ip_address(target.split(':')[0])  # Remove port if present
            for test_range in test_ranges:
                if ip in ipaddress.ip_network(test_range):
                    return True
                    
            # Check specific demo targets
            demo_targets = [
                "http://localhost:3000",  # Juice Shop
                "192.168.1.100",         # Metasploitable
                "demo.company.com"       # Phishing simulation
            ]
            
            if target in demo_targets:
                return True
                
        except Exception as e:
            self.logger.debug(f"Error in consent verification: {e}")
        
        return False
    
    async def check_target_allowed(self, target: str) -> bool:
        """Check if target is in the allowed list."""
        if not self.security_config.allowed_targets:
            # If no allowed targets specified, check against blocked targets
            return target not in self.security_config.blocked_targets
        
        # Check if target is explicitly allowed
        target_allowed = False
        
        for allowed_target in self.security_config.allowed_targets:
            if self._target_matches(target, allowed_target):
                target_allowed = True
                break
        
        # Also check blocked targets
        target_blocked = target in self.security_config.blocked_targets
        
        result = target_allowed and not target_blocked
        
        self._audit_log("TARGET_ACCESS_CHECK", {
            "target": target,
            "allowed": target_allowed,
            "blocked": target_blocked,
            "final_result": result
        })
        
        if not result:
            self.blocked_actions.append({
                "timestamp": datetime.now().isoformat(),
                "action": "target_access",
                "target": target,
                "reason": "Target not in allowed list or is blocked"
            })
        
        return result
    
    def _target_matches(self, target: str, pattern: str) -> bool:
        """Check if target matches an allowed pattern."""
        # Simple pattern matching - in production would support more complex patterns
        if pattern == "*":
            return True
        
        if pattern == target:
            return True
        
        # Check wildcard patterns
        if "*" in pattern:
            import fnmatch
            return fnmatch.fnmatch(target, pattern)
        
        # Check CIDR ranges for IP addresses
        try:
            if "/" in pattern:
                network = ipaddress.ip_network(pattern)
                target_ip = ipaddress.ip_address(target.split(':')[0])
                return target_ip in network
        except Exception:
            pass
        
        return False
    
    async def validate_action(self, action: str, context: Dict[str, Any]) -> bool:
        """Validate that an action is allowed under security policies."""
        if not self.security_config.enforce_zero_trust:
            return True
        
        self.logger.debug(f"Validating action: {action}")
        
        # Define restricted actions
        restricted_actions = [
            "destructive_exploit",
            "data_exfiltration",
            "lateral_movement",
            "persistence_installation",
            "privilege_escalation"
        ]
        
        # Check if action is restricted
        if action in restricted_actions:
            # Additional validation required for restricted actions
            if not await self._validate_restricted_action(action, context):
                self._audit_log("ACTION_BLOCKED", {
                    "action": action,
                    "context": context,
                    "reason": "Restricted action failed validation"
                })
                return False
        
        self._audit_log("ACTION_VALIDATED", {
            "action": action,
            "context": context
        })
        
        return True
    
    async def _validate_restricted_action(self, action: str, context: Dict[str, Any]) -> bool:
        """Validate restricted actions with additional checks."""
        target = context.get("target")
        
        # For demo purposes, allow restricted actions on test targets only
        if target:
            return await self._mock_consent_verification(target, "system")
        
        return False
    
    async def log_assessment_completion(self, assessment_data: Optional[Dict[str, Any]]):
        """Log assessment completion for audit trail."""
        if not assessment_data:
            return
        
        self._audit_log("ASSESSMENT_COMPLETED", {
            "assessment_id": assessment_data.get("id"),
            "target": assessment_data.get("target"),
            "status": assessment_data.get("status"),
            "start_time": assessment_data.get("start_time"),
            "end_time": assessment_data.get("end_time"),
            "findings_count": len(assessment_data.get("results", {}).get("phases", []))
        })
    
    async def generate_compliance_report(self, framework: str = "SOC2") -> Dict[str, Any]:
        """Generate compliance report for specified framework."""
        if framework not in self.compliance_mappings:
            raise ValueError(f"Unsupported compliance framework: {framework}")
        
        report = {
            "framework": framework,
            "generated_at": datetime.now().isoformat(),
            "compliance_status": {},
            "audit_log_integrity": await self._verify_audit_log_integrity(),
            "security_controls": {}
        }
        
        # Map audit events to compliance controls
        controls = self.compliance_mappings[framework]
        
        for control_id, control_desc in controls.items():
            report["compliance_status"][control_id] = {
                "description": control_desc,
                "status": "IMPLEMENTED",  # Would check actual implementation
                "evidence": [],
                "recommendations": []
            }
        
        # Add security control evidence
        report["security_controls"] = {
            "zero_trust_enforcement": self.security_config.enforce_zero_trust,
            "consent_verification": self.security_config.require_consent,
            "audit_logging": self.security_config.audit_log_enabled,
            "access_controls": len(self.security_config.allowed_targets) > 0
        }
        
        # Count relevant audit events
        relevant_events = ["CONSENT_VERIFIED", "TARGET_ACCESS_CHECK", "ACTION_BLOCKED", "ASSESSMENT_COMPLETED"]
        event_counts = {}
        
        for event_type in relevant_events:
            count = len([log for log in self.audit_logs if log.get("event_type") == event_type])
            event_counts[event_type] = count
        
        report["audit_statistics"] = event_counts
        
        self._audit_log("COMPLIANCE_REPORT_GENERATED", {
            "framework": framework,
            "report_id": hashlib.sha256(json.dumps(report, sort_keys=True).encode()).hexdigest()[:16]
        })
        
        return report
    
    async def _verify_audit_log_integrity(self) -> Dict[str, Any]:
        """Verify the integrity of all audit logs."""
        total_logs = len(self.audit_logs)
        valid_logs = 0
        integrity_failures = []
        
        for log_entry in self.audit_logs:
            if self._verify_log_integrity(log_entry):
                valid_logs += 1
            else:
                integrity_failures.append(log_entry.get("event_id", "unknown"))
        
        return {
            "total_logs": total_logs,
            "valid_logs": valid_logs,
            "integrity_failures": integrity_failures,
            "integrity_percentage": (valid_logs / total_logs * 100) if total_logs > 0 else 100
        }
    
    async def get_status(self) -> Dict[str, Any]:
        """Get current compliance guardian status."""
        return {
            "zero_trust_enabled": self.security_config.enforce_zero_trust,
            "consent_required": self.security_config.require_consent,
            "audit_log_enabled": self.security_config.audit_log_enabled,
            "total_audit_logs": len(self.audit_logs),
            "consent_records": len(self.consent_records),
            "blocked_actions": len(self.blocked_actions),
            "compliance_frameworks": self.security_config.compliance_frameworks,
            "allowed_targets": len(self.security_config.allowed_targets),
            "blocked_targets": len(self.security_config.blocked_targets)
        }