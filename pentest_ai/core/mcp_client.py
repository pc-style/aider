"""
MCP (Model Context Protocol) Client for Pentest AI

Provides unified interface to communicate with various LLM providers
including OpenAI, Claude, Gemini, Hugging Face, and local models.
"""

import asyncio
import logging
import json
import os
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from enum import Enum
import aiohttp
import openai
from anthropic import Anthropic
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


class LLMProvider(Enum):
    """Supported LLM providers"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    HUGGINGFACE = "huggingface"
    LOCAL = "local"


@dataclass
class LLMConfig:
    """Configuration for an LLM provider"""
    provider: LLMProvider
    api_key: Optional[str] = None
    model_name: str = ""
    base_url: Optional[str] = None
    max_tokens: int = 4096
    temperature: float = 0.7
    timeout: int = 60


@dataclass
class Message:
    """A message in the conversation"""
    role: str  # "system", "user", "assistant"
    content: str
    timestamp: Optional[str] = None


class MCPClient:
    """
    Model Context Protocol client for unified LLM communication
    """
    
    def __init__(self, config: Optional[Dict[str, LLMConfig]] = None):
        self.configs = config or {}
        self.logger = logging.getLogger(__name__)
        
        # Initialize default configurations
        self._initialize_default_configs()
        
        # Initialize clients
        self.clients = {}
        self._initialize_clients()
    
    def _initialize_default_configs(self):
        """Initialize default LLM configurations from environment"""
        
        # OpenAI
        if os.getenv("OPENAI_API_KEY"):
            self.configs[LLMProvider.OPENAI] = LLMConfig(
                provider=LLMProvider.OPENAI,
                api_key=os.getenv("OPENAI_API_KEY"),
                model_name=os.getenv("OPENAI_MODEL", "gpt-4"),
                base_url=os.getenv("OPENAI_BASE_URL"),
                max_tokens=int(os.getenv("OPENAI_MAX_TOKENS", "4096")),
                temperature=float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
            )
        
        # Anthropic (Claude)
        if os.getenv("ANTHROPIC_API_KEY"):
            self.configs[LLMProvider.ANTHROPIC] = LLMConfig(
                provider=LLMProvider.ANTHROPIC,
                api_key=os.getenv("ANTHROPIC_API_KEY"),
                model_name=os.getenv("ANTHROPIC_MODEL", "claude-3-sonnet-20240229"),
                max_tokens=int(os.getenv("ANTHROPIC_MAX_TOKENS", "4096")),
                temperature=float(os.getenv("ANTHROPIC_TEMPERATURE", "0.7"))
            )
        
        # Google (Gemini)
        if os.getenv("GOOGLE_API_KEY"):
            self.configs[LLMProvider.GOOGLE] = LLMConfig(
                provider=LLMProvider.GOOGLE,
                api_key=os.getenv("GOOGLE_API_KEY"),
                model_name=os.getenv("GOOGLE_MODEL", "gemini-pro"),
                max_tokens=int(os.getenv("GOOGLE_MAX_TOKENS", "4096")),
                temperature=float(os.getenv("GOOGLE_TEMPERATURE", "0.7"))
            )
        
        # Hugging Face
        if os.getenv("HUGGINGFACE_API_KEY"):
            self.configs[LLMProvider.HUGGINGFACE] = LLMConfig(
                provider=LLMProvider.HUGGINGFACE,
                api_key=os.getenv("HUGGINGFACE_API_KEY"),
                model_name=os.getenv("HUGGINGFACE_MODEL", "meta-llama/Llama-2-70b-chat-hf"),
                base_url=os.getenv("HUGGINGFACE_BASE_URL", "https://api-inference.huggingface.co"),
                max_tokens=int(os.getenv("HUGGINGFACE_MAX_TOKENS", "4096")),
                temperature=float(os.getenv("HUGGINGFACE_TEMPERATURE", "0.7"))
            )
        
        # Local models
        local_model_path = os.getenv("LOCAL_MODEL_PATH")
        if local_model_path:
            self.configs[LLMProvider.LOCAL] = LLMConfig(
                provider=LLMProvider.LOCAL,
                model_name=os.getenv("LOCAL_MODEL_NAME", "llama-2-7b-chat"),
                max_tokens=int(os.getenv("LOCAL_MAX_TOKENS", "4096")),
                temperature=float(os.getenv("LOCAL_TEMPERATURE", "0.7"))
            )
    
    def _initialize_clients(self):
        """Initialize client connections to LLM providers"""
        
        for provider, config in self.configs.items():
            try:
                if provider == LLMProvider.OPENAI:
                    self.clients[provider] = openai.AsyncOpenAI(
                        api_key=config.api_key,
                        base_url=config.base_url
                    )
                
                elif provider == LLMProvider.ANTHROPIC:
                    self.clients[provider] = Anthropic(
                        api_key=config.api_key
                    )
                
                elif provider == LLMProvider.GOOGLE:
                    genai.configure(api_key=config.api_key)
                    self.clients[provider] = genai.GenerativeModel(config.model_name)
                
                elif provider == LLMProvider.HUGGINGFACE:
                    # Hugging Face uses HTTP API
                    self.clients[provider] = None  # Will use aiohttp
                
                elif provider == LLMProvider.LOCAL:
                    # Initialize local model
                    self._initialize_local_model(config)
                
                self.logger.info(f"Initialized {provider.value} client")
                
            except Exception as e:
                self.logger.error(f"Failed to initialize {provider.value} client: {e}")
    
    def _initialize_local_model(self, config: LLMConfig):
        """Initialize a local model"""
        try:
            model_path = os.getenv("LOCAL_MODEL_PATH")
            if not model_path:
                raise ValueError("LOCAL_MODEL_PATH environment variable not set")
            
            # Load tokenizer and model
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            self.clients[LLMProvider.LOCAL] = {
                "tokenizer": tokenizer,
                "model": model,
                "config": config
            }
            
        except Exception as e:
            self.logger.error(f"Failed to initialize local model: {e}")
    
    async def send_message(
        self,
        message: str,
        provider: Optional[Union[str, LLMProvider]] = None,
        model: Optional[str] = None,
        system_prompt: Optional[str] = None,
        conversation_history: Optional[List[Message]] = None,
        timeout: Optional[int] = None
    ) -> str:
        """
        Send a message to an LLM and get the response
        
        Args:
            message: The user message
            provider: LLM provider to use (defaults to first available)
            model: Specific model to use
            system_prompt: System prompt to include
            conversation_history: Previous conversation messages
            timeout: Request timeout in seconds
        
        Returns:
            The LLM response
        """
        
        # Determine provider
        if provider is None:
            provider = next(iter(self.configs.keys()))
        elif isinstance(provider, str):
            provider = LLMProvider(provider)
        
        if provider not in self.configs:
            raise ValueError(f"Provider {provider.value} not configured")
        
        config = self.configs[provider]
        
        # Override model if specified
        if model:
            config = LLMConfig(
                provider=config.provider,
                api_key=config.api_key,
                model_name=model,
                base_url=config.base_url,
                max_tokens=config.max_tokens,
                temperature=config.temperature,
                timeout=timeout or config.timeout
            )
        
        # Build messages
        messages = []
        
        if system_prompt:
            messages.append(Message(role="system", content=system_prompt))
        
        if conversation_history:
            messages.extend(conversation_history)
        
        messages.append(Message(role="user", content=message))
        
        # Send to appropriate provider
        try:
            if provider == LLMProvider.OPENAI:
                return await self._send_openai(messages, config)
            
            elif provider == LLMProvider.ANTHROPIC:
                return await self._send_anthropic(messages, config)
            
            elif provider == LLMProvider.GOOGLE:
                return await self._send_google(messages, config)
            
            elif provider == LLMProvider.HUGGINGFACE:
                return await self._send_huggingface(messages, config)
            
            elif provider == LLMProvider.LOCAL:
                return await self._send_local(messages, config)
            
            else:
                raise ValueError(f"Unsupported provider: {provider.value}")
        
        except Exception as e:
            self.logger.error(f"Failed to send message to {provider.value}: {e}")
            raise
    
    async def _send_openai(self, messages: List[Message], config: LLMConfig) -> str:
        """Send message to OpenAI"""
        client = self.clients[LLMProvider.OPENAI]
        
        # Convert messages to OpenAI format
        openai_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]
        
        response = await client.chat.completions.create(
            model=config.model_name,
            messages=openai_messages,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            timeout=config.timeout
        )
        
        return response.choices[0].message.content
    
    async def _send_anthropic(self, messages: List[Message], config: LLMConfig) -> str:
        """Send message to Anthropic (Claude)"""
        client = self.clients[LLMProvider.ANTHROPIC]
        
        # Convert messages to Anthropic format
        system_message = None
        user_messages = []
        
        for msg in messages:
            if msg.role == "system":
                system_message = msg.content
            elif msg.role == "user":
                user_messages.append(msg.content)
            elif msg.role == "assistant":
                # Anthropic doesn't support assistant messages in the same way
                # We'll append to user message
                if user_messages:
                    user_messages[-1] += f"\n\nAssistant: {msg.content}"
        
        # Combine user messages
        user_content = "\n\n".join(user_messages)
        
        response = await client.messages.create(
            model=config.model_name,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            system=system_message,
            messages=[{"role": "user", "content": user_content}]
        )
        
        return response.content[0].text
    
    async def _send_google(self, messages: List[Message], config: LLMConfig) -> str:
        """Send message to Google (Gemini)"""
        model = self.clients[LLMProvider.GOOGLE]
        
        # Convert messages to Gemini format
        conversation = []
        for msg in messages:
            if msg.role == "user":
                conversation.append(msg.content)
            elif msg.role == "assistant":
                conversation.append(msg.content)
        
        # Gemini expects alternating user/assistant messages
        response = await model.generate_content(
            conversation,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=config.max_tokens,
                temperature=config.temperature
            )
        )
        
        return response.text
    
    async def _send_huggingface(self, messages: List[Message], config: LLMConfig) -> str:
        """Send message to Hugging Face"""
        
        # Prepare payload
        payload = {
            "inputs": self._format_huggingface_input(messages),
            "parameters": {
                "max_new_tokens": config.max_tokens,
                "temperature": config.temperature,
                "return_full_text": False
            }
        }
        
        headers = {
            "Authorization": f"Bearer {config.api_key}",
            "Content-Type": "application/json"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{config.base_url}/models/{config.model_name}",
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=config.timeout)
            ) as response:
                if response.status != 200:
                    raise Exception(f"Hugging Face API error: {response.status}")
                
                result = await response.json()
                return result[0]["generated_text"]
    
    def _format_huggingface_input(self, messages: List[Message]) -> str:
        """Format messages for Hugging Face API"""
        formatted = []
        
        for msg in messages:
            if msg.role == "system":
                formatted.append(f"<|system|>\n{msg.content}\n<|end|>")
            elif msg.role == "user":
                formatted.append(f"<|user|>\n{msg.content}\n<|end|>")
            elif msg.role == "assistant":
                formatted.append(f"<|assistant|>\n{msg.content}\n<|end|>")
        
        return "\n".join(formatted)
    
    async def _send_local(self, messages: List[Message], config: LLMConfig) -> str:
        """Send message to local model"""
        
        client_data = self.clients[LLMProvider.LOCAL]
        tokenizer = client_data["tokenizer"]
        model = client_data["model"]
        
        # Format input
        input_text = self._format_local_input(messages)
        
        # Tokenize
        inputs = tokenizer(input_text, return_tensors="pt")
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Decode
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract only the new tokens
        response = response[len(input_text):]
        
        return response.strip()
    
    def _format_local_input(self, messages: List[Message]) -> str:
        """Format messages for local model"""
        formatted = []
        
        for msg in messages:
            if msg.role == "system":
                formatted.append(f"<|system|>\n{msg.content}\n<|end|>")
            elif msg.role == "user":
                formatted.append(f"<|user|>\n{msg.content}\n<|end|>")
            elif msg.role == "assistant":
                formatted.append(f"<|assistant|>\n{msg.content}\n<|end|>")
        
        return "\n".join(formatted)
    
    async def get_available_providers(self) -> List[str]:
        """Get list of available LLM providers"""
        return [provider.value for provider in self.configs.keys()]
    
    async def get_provider_info(self, provider: Union[str, LLMProvider]) -> Dict[str, Any]:
        """Get information about a specific provider"""
        if isinstance(provider, str):
            provider = LLMProvider(provider)
        
        if provider not in self.configs:
            raise ValueError(f"Provider {provider.value} not configured")
        
        config = self.configs[provider]
        
        return {
            "provider": provider.value,
            "model_name": config.model_name,
            "max_tokens": config.max_tokens,
            "temperature": config.temperature,
            "timeout": config.timeout
        }
    
    async def test_connection(self, provider: Optional[Union[str, LLMProvider]] = None) -> bool:
        """Test connection to a specific provider"""
        try:
            if provider is None:
                provider = next(iter(self.configs.keys()))
            elif isinstance(provider, str):
                provider = LLMProvider(provider)
            
            # Send a simple test message
            response = await self.send_message(
                "Hello, this is a test message.",
                provider=provider,
                timeout=10
            )
            
            return bool(response and len(response.strip()) > 0)
        
        except Exception as e:
            self.logger.error(f"Connection test failed for {provider.value}: {e}")
            return False