"""
Multi-Agent Orchestrator for Pentest AI

Coordinates multiple AI agents to execute complex penetration testing workflows
with reinforcement learning feedback loops for optimization.
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum
import uuid
from datetime import datetime

from .mcp_client import MCPClient
from .graph_db import GraphDatabase
from .safety_gates import SafetyGates
from .audit_logger import AuditLogger


class AgentType(Enum):
    """Types of AI agents in the system"""
    RECONNAISSANCE = "reconnaissance"
    VULNERABILITY_SCANNER = "vulnerability_scanner"
    EXPLOITATION = "exploitation"
    POST_EXPLOITATION = "post_exploitation"
    REPORTING = "reporting"
    COORDINATOR = "coordinator"


class WorkflowStatus(Enum):
    """Status of workflow execution"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class AgentConfig:
    """Configuration for an AI agent"""
    agent_type: AgentType
    llm_provider: str
    model_name: str
    capabilities: List[str]
    max_concurrent_tasks: int = 3
    timeout_seconds: int = 300


@dataclass
class WorkflowStep:
    """A step in a pentest workflow"""
    step_id: str
    agent_type: AgentType
    description: str
    dependencies: List[str]
    parameters: Dict[str, Any]
    expected_output: str
    timeout_seconds: int = 300


@dataclass
class WorkflowResult:
    """Result of a workflow execution"""
    workflow_id: str
    status: WorkflowStatus
    start_time: datetime
    end_time: Optional[datetime]
    steps_completed: List[str]
    steps_failed: List[str]
    results: Dict[str, Any]
    feedback_score: Optional[float] = None


class MultiAgentOrchestrator:
    """
    Orchestrates multiple AI agents for complex pentest workflows
    with reinforcement learning feedback loops.
    """
    
    def __init__(
        self,
        mcp_client: MCPClient,
        graph_db: GraphDatabase,
        safety_gates: SafetyGates,
        audit_logger: AuditLogger,
        config: Optional[Dict[str, Any]] = None
    ):
        self.mcp_client = mcp_client
        self.graph_db = graph_db
        self.safety_gates = safety_gates
        self.audit_logger = audit_logger
        self.config = config or {}
        
        self.agents: Dict[str, AgentConfig] = {}
        self.active_workflows: Dict[str, WorkflowResult] = {}
        self.workflow_history: List[WorkflowResult] = []
        self.feedback_loop = FeedbackLoop()
        
        self.logger = logging.getLogger(__name__)
        
        # Initialize default agents
        self._initialize_default_agents()
    
    def _initialize_default_agents(self):
        """Initialize default AI agents"""
        default_agents = {
            "recon": AgentConfig(
                agent_type=AgentType.RECONNAISSANCE,
                llm_provider="openai",
                model_name="gpt-4",
                capabilities=["port_scanning", "subdomain_enumeration", "service_detection"]
            ),
            "vuln_scanner": AgentConfig(
                agent_type=AgentType.VULNERABILITY_SCANNER,
                llm_provider="openai", 
                model_name="gpt-4",
                capabilities=["vulnerability_analysis", "cve_matching", "risk_assessment"]
            ),
            "exploitation": AgentConfig(
                agent_type=AgentType.EXPLOITATION,
                llm_provider="openai",
                model_name="gpt-4",
                capabilities=["exploit_selection", "payload_generation", "privilege_escalation"]
            ),
            "post_exploit": AgentConfig(
                agent_type=AgentType.POST_EXPLOITATION,
                llm_provider="openai",
                model_name="gpt-4",
                capabilities=["data_extraction", "persistence", "lateral_movement"]
            ),
            "reporter": AgentConfig(
                agent_type=AgentType.REPORTING,
                llm_provider="openai",
                model_name="gpt-4",
                capabilities=["report_generation", "risk_scoring", "mitigation_suggestions"]
            )
        }
        
        for agent_id, config in default_agents.items():
            self.register_agent(agent_id, config)
    
    def register_agent(self, agent_id: str, config: AgentConfig):
        """Register a new AI agent"""
        self.agents[agent_id] = config
        self.logger.info(f"Registered agent: {agent_id} ({config.agent_type.value})")
        
        # Log to audit trail
        self.audit_logger.log_event(
            event_type="agent_registered",
            agent_id=agent_id,
            agent_type=config.agent_type.value,
            details=config.__dict__
        )
    
    async def create_workflow(
        self,
        target: str,
        scope: List[str],
        workflow_type: str,
        parameters: Optional[Dict[str, Any]] = None
    ) -> str:
        """Create a new pentest workflow"""
        workflow_id = str(uuid.uuid4())
        
        # Safety check
        if not await self.safety_gates.verify_consent(target, scope):
            raise ValueError("Consent verification failed for target")
        
        # Create workflow steps based on type and scope
        steps = await self._generate_workflow_steps(workflow_type, scope, parameters)
        
        # Store workflow in graph database
        await self.graph_db.create_workflow(
            workflow_id=workflow_id,
            target=target,
            scope=scope,
            workflow_type=workflow_type,
            steps=steps
        )
        
        # Initialize workflow result
        workflow_result = WorkflowResult(
            workflow_id=workflow_id,
            status=WorkflowStatus.PENDING,
            start_time=datetime.utcnow(),
            end_time=None,
            steps_completed=[],
            steps_failed=[],
            results={}
        )
        
        self.active_workflows[workflow_id] = workflow_result
        
        self.logger.info(f"Created workflow {workflow_id} for target {target}")
        
        # Audit log
        self.audit_logger.log_event(
            event_type="workflow_created",
            workflow_id=workflow_id,
            target=target,
            scope=scope,
            workflow_type=workflow_type
        )
        
        return workflow_id
    
    async def _generate_workflow_steps(
        self,
        workflow_type: str,
        scope: List[str],
        parameters: Optional[Dict[str, Any]] = None
    ) -> List[WorkflowStep]:
        """Generate workflow steps using AI"""
        
        # Use MCP to get AI suggestions for workflow steps
        prompt = f"""
        Generate a penetration testing workflow for type '{workflow_type}' with scope {scope}.
        Consider the following parameters: {parameters or {}}
        
        Return a JSON array of workflow steps with the following structure:
        {{
            "step_id": "unique_step_id",
            "agent_type": "reconnaissance|vulnerability_scanner|exploitation|post_exploitation|reporting",
            "description": "Step description",
            "dependencies": ["list", "of", "step", "ids"],
            "parameters": {{"param1": "value1"}},
            "expected_output": "Expected output description",
            "timeout_seconds": 300
        }}
        """
        
        response = await self.mcp_client.send_message(prompt)
        
        # Parse AI response and create workflow steps
        steps = []
        try:
            # Parse JSON response and create WorkflowStep objects
            # This is a simplified version - in practice, you'd have proper JSON parsing
            steps = self._parse_workflow_steps(response)
        except Exception as e:
            self.logger.error(f"Failed to parse workflow steps: {e}")
            # Fallback to default steps
            steps = self._get_default_workflow_steps(workflow_type, scope)
        
        return steps
    
    def _parse_workflow_steps(self, ai_response: str) -> List[WorkflowStep]:
        """Parse AI-generated workflow steps"""
        # Implementation would parse JSON from AI response
        # For now, return default steps
        return self._get_default_workflow_steps("web", ["web"])
    
    def _get_default_workflow_steps(
        self,
        workflow_type: str,
        scope: List[str]
    ) -> List[WorkflowStep]:
        """Get default workflow steps for common pentest types"""
        
        if workflow_type == "web":
            return [
                WorkflowStep(
                    step_id="recon_1",
                    agent_type=AgentType.RECONNAISSANCE,
                    description="Initial reconnaissance and port scanning",
                    dependencies=[],
                    parameters={"scan_type": "comprehensive"},
                    expected_output="Port scan results and service detection"
                ),
                WorkflowStep(
                    step_id="vuln_1",
                    agent_type=AgentType.VULNERABILITY_SCANNER,
                    description="Vulnerability scanning and analysis",
                    dependencies=["recon_1"],
                    parameters={"tools": ["nmap", "nikto", "sqlmap"]},
                    expected_output="Vulnerability assessment report"
                ),
                WorkflowStep(
                    step_id="exploit_1",
                    agent_type=AgentType.EXPLOITATION,
                    description="Exploitation of identified vulnerabilities",
                    dependencies=["vuln_1"],
                    parameters={"exploit_type": "web"},
                    expected_output="Exploitation results and access gained"
                ),
                WorkflowStep(
                    step_id="report_1",
                    agent_type=AgentType.REPORTING,
                    description="Generate comprehensive report",
                    dependencies=["exploit_1"],
                    parameters={"format": "html"},
                    expected_output="Final pentest report"
                )
            ]
        
        # Add more workflow types as needed
        return []
    
    async def execute_workflow(self, workflow_id: str) -> WorkflowResult:
        """Execute a pentest workflow"""
        
        if workflow_id not in self.active_workflows:
            raise ValueError(f"Workflow {workflow_id} not found")
        
        workflow_result = self.active_workflows[workflow_id]
        workflow_result.status = WorkflowStatus.RUNNING
        
        try:
            # Get workflow from database
            workflow_data = await self.graph_db.get_workflow(workflow_id)
            steps = workflow_data["steps"]
            
            # Execute steps in dependency order
            completed_steps = []
            failed_steps = []
            results = {}
            
            for step in steps:
                try:
                    # Check dependencies
                    if not all(dep in completed_steps for dep in step.dependencies):
                        continue
                    
                    # Execute step
                    step_result = await self._execute_workflow_step(step, results)
                    results[step.step_id] = step_result
                    completed_steps.append(step.step_id)
                    
                    # Update workflow status
                    workflow_result.steps_completed = completed_steps
                    workflow_result.results = results
                    
                except Exception as e:
                    self.logger.error(f"Step {step.step_id} failed: {e}")
                    failed_steps.append(step.step_id)
                    workflow_result.steps_failed = failed_steps
            
            # Determine final status
            if failed_steps:
                workflow_result.status = WorkflowStatus.FAILED
            else:
                workflow_result.status = WorkflowStatus.COMPLETED
            
            workflow_result.end_time = datetime.utcnow()
            
            # Apply reinforcement learning feedback
            await self.feedback_loop.update_model(workflow_result)
            
            # Store results in database
            await self.graph_db.update_workflow_results(
                workflow_id=workflow_id,
                results=results,
                status=workflow_result.status.value
            )
            
        except Exception as e:
            self.logger.error(f"Workflow {workflow_id} execution failed: {e}")
            workflow_result.status = WorkflowStatus.FAILED
            workflow_result.end_time = datetime.utcnow()
        
        # Move to history
        self.workflow_history.append(workflow_result)
        del self.active_workflows[workflow_id]
        
        # Audit log
        self.audit_logger.log_event(
            event_type="workflow_completed",
            workflow_id=workflow_id,
            status=workflow_result.status.value,
            steps_completed=len(workflow_result.steps_completed),
            steps_failed=len(workflow_result.steps_failed)
        )
        
        return workflow_result
    
    async def _execute_workflow_step(
        self,
        step: WorkflowStep,
        previous_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute a single workflow step"""
        
        # Find appropriate agent
        agent_id = self._find_agent_for_step(step.agent_type)
        if not agent_id:
            raise ValueError(f"No agent found for step type {step.agent_type}")
        
        agent_config = self.agents[agent_id]
        
        # Prepare context from previous results
        context = self._prepare_step_context(step, previous_results)
        
        # Execute step using MCP
        prompt = f"""
        Execute the following pentest step:
        
        Step: {step.description}
        Agent Type: {step.agent_type.value}
        Parameters: {step.parameters}
        Context: {context}
        
        Expected Output: {step.expected_output}
        
        Provide your analysis and recommendations in a structured format.
        """
        
        response = await self.mcp_client.send_message(
            prompt,
            provider=agent_config.llm_provider,
            model=agent_config.model_name,
            timeout=step.timeout_seconds
        )
        
        return {
            "step_id": step.step_id,
            "agent_id": agent_id,
            "output": response,
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed"
        }
    
    def _find_agent_for_step(self, agent_type: AgentType) -> Optional[str]:
        """Find the best agent for a given step type"""
        for agent_id, config in self.agents.items():
            if config.agent_type == agent_type:
                return agent_id
        return None
    
    def _prepare_step_context(
        self,
        step: WorkflowStep,
        previous_results: Dict[str, Any]
    ) -> str:
        """Prepare context for a workflow step"""
        context_parts = []
        
        for dep in step.dependencies:
            if dep in previous_results:
                result = previous_results[dep]
                context_parts.append(f"Step {dep}: {result.get('output', 'N/A')}")
        
        return "\n".join(context_parts) if context_parts else "No previous context"
    
    async def get_workflow_status(self, workflow_id: str) -> Optional[WorkflowResult]:
        """Get the status of a workflow"""
        if workflow_id in self.active_workflows:
            return self.active_workflows[workflow_id]
        
        # Check history
        for result in self.workflow_history:
            if result.workflow_id == workflow_id:
                return result
        
        return None
    
    async def cancel_workflow(self, workflow_id: str) -> bool:
        """Cancel a running workflow"""
        if workflow_id not in self.active_workflows:
            return False
        
        workflow_result = self.active_workflows[workflow_id]
        workflow_result.status = WorkflowStatus.CANCELLED
        workflow_result.end_time = datetime.utcnow()
        
        # Move to history
        self.workflow_history.append(workflow_result)
        del self.active_workflows[workflow_id]
        
        self.logger.info(f"Cancelled workflow {workflow_id}")
        
        # Audit log
        self.audit_logger.log_event(
            event_type="workflow_cancelled",
            workflow_id=workflow_id
        )
        
        return True


class FeedbackLoop:
    """Reinforcement learning feedback loop for optimizing workflows"""
    
    def __init__(self):
        self.performance_history = []
        self.optimization_rules = []
    
    async def update_model(self, workflow_result: WorkflowResult):
        """Update the model based on workflow results"""
        # Calculate performance score
        score = self._calculate_performance_score(workflow_result)
        workflow_result.feedback_score = score
        
        # Store performance data
        self.performance_history.append({
            "workflow_id": workflow_result.workflow_id,
            "score": score,
            "steps_completed": len(workflow_result.steps_completed),
            "steps_failed": len(workflow_result.steps_failed),
            "duration": (workflow_result.end_time - workflow_result.start_time).total_seconds()
        })
        
        # Update optimization rules based on performance
        self._update_optimization_rules(workflow_result, score)
    
    def _calculate_performance_score(self, workflow_result: WorkflowResult) -> float:
        """Calculate a performance score for the workflow"""
        if workflow_result.status == WorkflowStatus.FAILED:
            return 0.0
        
        # Base score from completion
        base_score = len(workflow_result.steps_completed) / (
            len(workflow_result.steps_completed) + len(workflow_result.steps_failed)
        )
        
        # Time efficiency bonus
        if workflow_result.end_time and workflow_result.start_time:
            duration = (workflow_result.end_time - workflow_result.start_time).total_seconds()
            time_efficiency = max(0, 1 - (duration / 3600))  # Penalize long workflows
            return (base_score + time_efficiency) / 2
        
        return base_score
    
    def _update_optimization_rules(self, workflow_result: WorkflowResult, score: float):
        """Update optimization rules based on performance"""
        # This would implement actual RL algorithms
        # For now, just store the performance data
        pass