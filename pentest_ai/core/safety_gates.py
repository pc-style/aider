"""
Safety Gates for Pentest AI

Implements zero-trust safety gates and consent verification
to ensure ethical and authorized penetration testing.
"""

import asyncio
import logging
import json
import os
import hashlib
import hmac
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
import ipaddress
import re
import yaml


class SafetyLevel(Enum):
    """Safety levels for operations"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ConsentStatus(Enum):
    """Consent verification status"""
    PENDING = "pending"
    VERIFIED = "verified"
    DENIED = "denied"
    EXPIRED = "expired"


@dataclass
class SafetyRule:
    """A safety rule for operations"""
    id: str
    name: str
    description: str
    level: SafetyLevel
    conditions: Dict[str, Any]
    actions: List[str]
    enabled: bool = True


@dataclass
class ConsentRecord:
    """A consent record for a target"""
    id: str
    target: str
    scope: List[str]
    user_id: str
    granted_at: datetime
    expires_at: datetime
    status: ConsentStatus
    verification_hash: str
    metadata: Dict[str, Any] = None


@dataclass
class SafetyCheck:
    """Result of a safety check"""
    passed: bool
    level: SafetyLevel
    message: str
    details: Dict[str, Any]
    timestamp: datetime


class SafetyGates:
    """
    Zero-trust safety gates for pentest operations
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # Safety rules
        self.safety_rules: Dict[str, SafetyRule] = {}
        
        # Consent records
        self.consent_records: Dict[str, ConsentRecord] = {}
        
        # Blocked targets and patterns
        self.blocked_targets: set = set()
        self.blocked_patterns: List[re.Pattern] = []
        
        # Initialize safety rules
        self._initialize_safety_rules()
        
        # Load blocked targets
        self._load_blocked_targets()
    
    def _initialize_safety_rules(self):
        """Initialize default safety rules"""
        
        default_rules = [
            SafetyRule(
                id="consent_verification",
                name="Consent Verification",
                description="Verify explicit consent for target",
                level=SafetyLevel.CRITICAL,
                conditions={
                    "require_consent": True,
                    "consent_expiry_hours": 24
                },
                actions=["block", "log", "alert"]
            ),
            SafetyRule(
                id="target_validation",
                name="Target Validation",
                description="Validate target is not in blocked list",
                level=SafetyLevel.HIGH,
                conditions={
                    "check_blocked_list": True,
                    "check_patterns": True
                },
                actions=["block", "log"]
            ),
            SafetyRule(
                id="scope_validation",
                name="Scope Validation",
                description="Validate operation is within authorized scope",
                level=SafetyLevel.HIGH,
                conditions={
                    "check_scope": True,
                    "max_scope_items": 100
                },
                actions=["block", "log"]
            ),
            SafetyRule(
                id="rate_limiting",
                name="Rate Limiting",
                description="Limit operations per time period",
                level=SafetyLevel.MEDIUM,
                conditions={
                    "max_operations_per_hour": 50,
                    "max_operations_per_day": 500
                },
                actions=["throttle", "log"]
            ),
            SafetyRule(
                id="sensitive_data_protection",
                name="Sensitive Data Protection",
                description="Protect sensitive data and credentials",
                level=SafetyLevel.CRITICAL,
                conditions={
                    "encrypt_credentials": True,
                    "mask_sensitive_data": True,
                    "audit_data_access": True
                },
                actions=["encrypt", "mask", "audit"]
            ),
            SafetyRule(
                id="network_isolation",
                name="Network Isolation",
                description="Ensure operations are properly isolated",
                level=SafetyLevel.HIGH,
                conditions={
                    "require_container_isolation": True,
                    "check_network_access": True
                },
                actions=["isolate", "log"]
            ),
            SafetyRule(
                id="tool_validation",
                name="Tool Validation",
                description="Validate tools are safe and authorized",
                level=SafetyLevel.MEDIUM,
                conditions={
                    "check_tool_signatures": True,
                    "validate_tool_versions": True
                },
                actions=["validate", "log"]
            ),
            SafetyRule(
                id="output_validation",
                name="Output Validation",
                description="Validate output doesn't contain sensitive data",
                level=SafetyLevel.MEDIUM,
                conditions={
                    "scan_for_credentials": True,
                    "check_for_pii": True
                },
                actions=["sanitize", "log"]
            )
        ]
        
        for rule in default_rules:
            self.safety_rules[rule.id] = rule
    
    def _load_blocked_targets(self):
        """Load blocked targets from configuration"""
        
        # Load from environment
        blocked_env = os.getenv("PENTEST_AI_BLOCKED_TARGETS", "")
        if blocked_env:
            self.blocked_targets.update(blocked_env.split(","))
        
        # Load from config file
        config_file = self.config.get("blocked_targets_file")
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    blocked_data = yaml.safe_load(f)
                    if blocked_data:
                        self.blocked_targets.update(blocked_data.get("targets", []))
                        patterns = blocked_data.get("patterns", [])
                        for pattern in patterns:
                            self.blocked_patterns.append(re.compile(pattern))
            except Exception as e:
                self.logger.error(f"Failed to load blocked targets: {e}")
        
        # Add default blocked targets
        default_blocked = [
            "localhost",
            "127.0.0.1",
            "::1",
            "0.0.0.0",
            "255.255.255.255"
        ]
        self.blocked_targets.update(default_blocked)
        
        self.logger.info(f"Loaded {len(self.blocked_targets)} blocked targets and {len(self.blocked_patterns)} patterns")
    
    async def verify_consent(
        self,
        target: str,
        scope: List[str],
        user_id: str,
        consent_hash: Optional[str] = None
    ) -> bool:
        """
        Verify consent for a target and scope
        
        Args:
            target: Target to verify consent for
            scope: Scope of operations
            user_id: User requesting consent
            consent_hash: Optional consent verification hash
        
        Returns:
            True if consent is verified, False otherwise
        """
        
        try:
            # Check for existing consent record
            consent_id = self._generate_consent_id(target, scope, user_id)
            
            if consent_id in self.consent_records:
                record = self.consent_records[consent_id]
                
                # Check if consent is still valid
                if record.status == ConsentStatus.VERIFIED and record.expires_at > datetime.utcnow():
                    return True
                elif record.status == ConsentStatus.DENIED:
                    return False
                elif record.expires_at <= datetime.utcnow():
                    record.status = ConsentStatus.EXPIRED
            
            # If consent hash provided, verify it
            if consent_hash:
                expected_hash = self._generate_consent_hash(target, scope, user_id)
                if hmac.compare_digest(consent_hash, expected_hash):
                    # Create consent record
                    consent_record = ConsentRecord(
                        id=consent_id,
                        target=target,
                        scope=scope,
                        user_id=user_id,
                        granted_at=datetime.utcnow(),
                        expires_at=datetime.utcnow() + timedelta(hours=24),
                        status=ConsentStatus.VERIFIED,
                        verification_hash=consent_hash
                    )
                    
                    self.consent_records[consent_id] = consent_record
                    return True
            
            # For critical operations, require explicit consent
            if self._is_critical_operation(scope):
                self.logger.warning(f"Critical operation requires explicit consent for target {target}")
                return False
            
            # For non-critical operations, check if target is in safe list
            if self._is_safe_target(target):
                return True
            
            self.logger.warning(f"No valid consent found for target {target}")
            return False
        
        except Exception as e:
            self.logger.error(f"Failed to verify consent for target {target}: {e}")
            return False
    
    def _generate_consent_id(self, target: str, scope: List[str], user_id: str) -> str:
        """Generate a unique consent ID"""
        consent_string = f"{target}:{':'.join(sorted(scope))}:{user_id}"
        return hashlib.sha256(consent_string.encode()).hexdigest()
    
    def _generate_consent_hash(self, target: str, scope: List[str], user_id: str) -> str:
        """Generate a consent verification hash"""
        consent_string = f"{target}:{':'.join(sorted(scope))}:{user_id}"
        secret = os.getenv("PENTEST_AI_CONSENT_SECRET", "default-secret")
        return hmac.new(secret.encode(), consent_string.encode(), hashlib.sha256).hexdigest()
    
    def _is_critical_operation(self, scope: List[str]) -> bool:
        """Check if operation is critical"""
        critical_operations = [
            "exploitation",
            "post_exploitation",
            "data_extraction",
            "privilege_escalation",
            "lateral_movement"
        ]
        
        return any(op in scope for op in critical_operations)
    
    def _is_safe_target(self, target: str) -> bool:
        """Check if target is in safe list"""
        safe_targets = self.config.get("safe_targets", [])
        return target in safe_targets
    
    async def validate_target(self, target: str) -> SafetyCheck:
        """
        Validate a target against safety rules
        
        Args:
            target: Target to validate
        
        Returns:
            SafetyCheck result
        """
        
        try:
            # Check blocked targets
            if target in self.blocked_targets:
                return SafetyCheck(
                    passed=False,
                    level=SafetyLevel.CRITICAL,
                    message=f"Target {target} is in blocked list",
                    details={"target": target, "blocked": True},
                    timestamp=datetime.utcnow()
                )
            
            # Check blocked patterns
            for pattern in self.blocked_patterns:
                if pattern.match(target):
                    return SafetyCheck(
                        passed=False,
                        level=SafetyLevel.CRITICAL,
                        message=f"Target {target} matches blocked pattern {pattern.pattern}",
                        details={"target": target, "pattern": pattern.pattern},
                        timestamp=datetime.utcnow()
                    )
            
            # Validate IP address format
            try:
                ipaddress.ip_address(target)
            except ValueError:
                # Not an IP address, check if it's a valid domain
                if not self._is_valid_domain(target):
                    return SafetyCheck(
                        passed=False,
                        level=SafetyLevel.HIGH,
                        message=f"Target {target} is not a valid IP address or domain",
                        details={"target": target, "valid_format": False},
                        timestamp=datetime.utcnow()
                    )
            
            return SafetyCheck(
                passed=True,
                level=SafetyLevel.LOW,
                message=f"Target {target} validation passed",
                details={"target": target, "valid": True},
                timestamp=datetime.utcnow()
            )
        
        except Exception as e:
            self.logger.error(f"Failed to validate target {target}: {e}")
            return SafetyCheck(
                passed=False,
                level=SafetyLevel.CRITICAL,
                message=f"Target validation failed: {e}",
                details={"target": target, "error": str(e)},
                timestamp=datetime.utcnow()
            )
    
    def _is_valid_domain(self, domain: str) -> bool:
        """Check if domain is valid"""
        # Basic domain validation
        domain_pattern = re.compile(
            r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$'
        )
        return bool(domain_pattern.match(domain))
    
    async def validate_scope(self, scope: List[str]) -> SafetyCheck:
        """
        Validate operation scope
        
        Args:
            scope: List of scope items
        
        Returns:
            SafetyCheck result
        """
        
        try:
            # Check scope size
            max_scope_items = self.safety_rules["scope_validation"].conditions.get("max_scope_items", 100)
            
            if len(scope) > max_scope_items:
                return SafetyCheck(
                    passed=False,
                    level=SafetyLevel.HIGH,
                    message=f"Scope too large: {len(scope)} items (max {max_scope_items})",
                    details={"scope_size": len(scope), "max_size": max_scope_items},
                    timestamp=datetime.utcnow()
                )
            
            # Check for dangerous operations
            dangerous_operations = [
                "ddos",
                "destructive",
                "data_destruction",
                "system_damage"
            ]
            
            for operation in scope:
                if operation in dangerous_operations:
                    return SafetyCheck(
                        passed=False,
                        level=SafetyLevel.CRITICAL,
                        message=f"Dangerous operation detected: {operation}",
                        details={"operation": operation, "dangerous": True},
                        timestamp=datetime.utcnow()
                    )
            
            return SafetyCheck(
                passed=True,
                level=SafetyLevel.LOW,
                message=f"Scope validation passed: {len(scope)} items",
                details={"scope_size": len(scope), "valid": True},
                timestamp=datetime.utcnow()
            )
        
        except Exception as e:
            self.logger.error(f"Failed to validate scope: {e}")
            return SafetyCheck(
                passed=False,
                level=SafetyLevel.CRITICAL,
                message=f"Scope validation failed: {e}",
                details={"error": str(e)},
                timestamp=datetime.utcnow()
            )
    
    async def validate_tools(self, tools: List[str]) -> SafetyCheck:
        """
        Validate tools for safety
        
        Args:
            tools: List of tools to validate
        
        Returns:
            SafetyCheck result
        """
        
        try:
            # Check for dangerous tools
            dangerous_tools = [
                "nuclear",
                "destructive",
                "malware",
                "virus"
            ]
            
            for tool in tools:
                if any(dangerous in tool.lower() for dangerous in dangerous_tools):
                    return SafetyCheck(
                        passed=False,
                        level=SafetyLevel.CRITICAL,
                        message=f"Dangerous tool detected: {tool}",
                        details={"tool": tool, "dangerous": True},
                        timestamp=datetime.utcnow()
                    )
            
            # Validate tool signatures if enabled
            if self.safety_rules["tool_validation"].conditions.get("check_tool_signatures", True):
                for tool in tools:
                    if not await self._validate_tool_signature(tool):
                        return SafetyCheck(
                            passed=False,
                            level=SafetyLevel.HIGH,
                            message=f"Tool signature validation failed: {tool}",
                            details={"tool": tool, "signature_valid": False},
                            timestamp=datetime.utcnow()
                        )
            
            return SafetyCheck(
                passed=True,
                level=SafetyLevel.LOW,
                message=f"Tool validation passed: {len(tools)} tools",
                details={"tools": tools, "valid": True},
                timestamp=datetime.utcnow()
            )
        
        except Exception as e:
            self.logger.error(f"Failed to validate tools: {e}")
            return SafetyCheck(
                passed=False,
                level=SafetyLevel.CRITICAL,
                message=f"Tool validation failed: {e}",
                details={"error": str(e)},
                timestamp=datetime.utcnow()
            )
    
    async def _validate_tool_signature(self, tool: str) -> bool:
        """Validate tool signature (placeholder implementation)"""
        # In a real implementation, this would check tool signatures
        # For now, just return True for known safe tools
        safe_tools = [
            "nmap", "sqlmap", "nikto", "gobuster", "hydra", "john", "hashcat"
        ]
        return tool.lower() in safe_tools
    
    async def sanitize_output(self, output: str) -> str:
        """
        Sanitize output to remove sensitive data
        
        Args:
            output: Output to sanitize
        
        Returns:
            Sanitized output
        """
        
        try:
            sanitized = output
            
            # Remove potential credentials
            credential_patterns = [
                r'password\s*[:=]\s*\S+',
                r'passwd\s*[:=]\s*\S+',
                r'pwd\s*[:=]\s*\S+',
                r'secret\s*[:=]\s*\S+',
                r'key\s*[:=]\s*\S+',
                r'token\s*[:=]\s*\S+'
            ]
            
            for pattern in credential_patterns:
                sanitized = re.sub(pattern, r'\1: [REDACTED]', sanitized, flags=re.IGNORECASE)
            
            # Remove potential IP addresses (except localhost)
            ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
            sanitized = re.sub(ip_pattern, '[IP_REDACTED]', sanitized)
            
            # Remove potential email addresses
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            sanitized = re.sub(email_pattern, '[EMAIL_REDACTED]', sanitized)
            
            return sanitized
        
        except Exception as e:
            self.logger.error(f"Failed to sanitize output: {e}")
            return output
    
    async def check_rate_limit(self, user_id: str, operation_type: str) -> SafetyCheck:
        """
        Check rate limiting for user
        
        Args:
            user_id: User ID
            operation_type: Type of operation
        
        Returns:
            SafetyCheck result
        """
        
        try:
            # Get rate limit rules
            rule = self.safety_rules["rate_limiting"]
            max_per_hour = rule.conditions.get("max_operations_per_hour", 50)
            max_per_day = rule.conditions.get("max_operations_per_day", 500)
            
            # Check current usage (simplified implementation)
            # In a real implementation, this would check against a database
            current_hour_ops = 0  # Would be retrieved from database
            current_day_ops = 0   # Would be retrieved from database
            
            if current_hour_ops >= max_per_hour:
                return SafetyCheck(
                    passed=False,
                    level=SafetyLevel.MEDIUM,
                    message=f"Hourly rate limit exceeded: {current_hour_ops}/{max_per_hour}",
                    details={"current": current_hour_ops, "limit": max_per_hour, "period": "hour"},
                    timestamp=datetime.utcnow()
                )
            
            if current_day_ops >= max_per_day:
                return SafetyCheck(
                    passed=False,
                    level=SafetyLevel.MEDIUM,
                    message=f"Daily rate limit exceeded: {current_day_ops}/{max_per_day}",
                    details={"current": current_day_ops, "limit": max_per_day, "period": "day"},
                    timestamp=datetime.utcnow()
                )
            
            return SafetyCheck(
                passed=True,
                level=SafetyLevel.LOW,
                message="Rate limit check passed",
                details={"hourly": current_hour_ops, "daily": current_day_ops},
                timestamp=datetime.utcnow()
            )
        
        except Exception as e:
            self.logger.error(f"Failed to check rate limit: {e}")
            return SafetyCheck(
                passed=False,
                level=SafetyLevel.CRITICAL,
                message=f"Rate limit check failed: {e}",
                details={"error": str(e)},
                timestamp=datetime.utcnow()
            )
    
    async def comprehensive_safety_check(
        self,
        target: str,
        scope: List[str],
        tools: List[str],
        user_id: str,
        operation_type: str
    ) -> List[SafetyCheck]:
        """
        Perform comprehensive safety checks
        
        Args:
            target: Target to check
            scope: Operation scope
            tools: Tools to use
            user_id: User ID
            operation_type: Type of operation
        
        Returns:
            List of safety check results
        """
        
        checks = []
        
        # Verify consent
        consent_verified = await self.verify_consent(target, scope, user_id)
        checks.append(SafetyCheck(
            passed=consent_verified,
            level=SafetyLevel.CRITICAL,
            message="Consent verification",
            details={"verified": consent_verified},
            timestamp=datetime.utcnow()
        ))
        
        # Validate target
        target_check = await self.validate_target(target)
        checks.append(target_check)
        
        # Validate scope
        scope_check = await self.validate_scope(scope)
        checks.append(scope_check)
        
        # Validate tools
        tools_check = await self.validate_tools(tools)
        checks.append(tools_check)
        
        # Check rate limits
        rate_check = await self.check_rate_limit(user_id, operation_type)
        checks.append(rate_check)
        
        return checks
    
    def add_safety_rule(self, rule: SafetyRule):
        """Add a new safety rule"""
        self.safety_rules[rule.id] = rule
        self.logger.info(f"Added safety rule: {rule.name}")
    
    def remove_safety_rule(self, rule_id: str):
        """Remove a safety rule"""
        if rule_id in self.safety_rules:
            del self.safety_rules[rule_id]
            self.logger.info(f"Removed safety rule: {rule_id}")
    
    def get_safety_rules(self) -> List[SafetyRule]:
        """Get all safety rules"""
        return list(self.safety_rules.values())
    
    def get_consent_records(self) -> List[ConsentRecord]:
        """Get all consent records"""
        return list(self.consent_records.values())
    
    def clear_expired_consent(self):
        """Clear expired consent records"""
        current_time = datetime.utcnow()
        expired_ids = [
            consent_id for consent_id, record in self.consent_records.items()
            if record.expires_at <= current_time
        ]
        
        for consent_id in expired_ids:
            del self.consent_records[consent_id]
        
        if expired_ids:
            self.logger.info(f"Cleared {len(expired_ids)} expired consent records")